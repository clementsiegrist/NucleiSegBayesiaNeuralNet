{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYrd5bXWVDZurQXDlv/grU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clementsiegrist/segmentation_probabilisticDL/blob/main/clem_bayesiannet_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHrWL9lLlbUj",
        "outputId": "df96c8d3-3abd-404a-eef3-8fd3a24dd098"
      },
      "source": [
        "pip install keras opencv-python tensorflow scikit-learn scikit-image numpy pillow matplotlib imutils"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (51.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAKg_9celkeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a818359-0243-435f-e1b5-fd1eecdac3fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0eje3gVMDzF",
        "outputId": "088a84d2-143f-48ec-c611-cb4d4aa37a1d"
      },
      "source": [
        "!git clone https://github.com/sandialabs/bcnn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bcnn'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 117 (delta 63), reused 67 (delta 31), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (117/117), 743.27 KiB | 11.99 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gfn86FXxNCc8",
        "outputId": "86821446-a483-4b24-946d-2d7d1924e251"
      },
      "source": [
        "!pip install tensorflow-gpu==1.12.0 tensorflow-probability==0.5.0 matplotlib==3.1.0 colorcet==2.0.1 brokenaxes==0.3.1 sacred==0.7.5 pymongo==3.8.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/7e/bec4d62e9dc95e828922c6cec38acd9461af8abe749f7c9def25ec4b2fdb/tensorflow_gpu-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (281.7MB)\n",
            "\u001b[K     |████████████████████████████████| 281.7MB 56kB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ca/6f213618b5f7d0bf6139e6ec928d412a5ca14e4776adfd41a59c74a34021/tensorflow_probability-0.5.0-py2.py3-none-any.whl (680kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 54.7MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 259kB/s \n",
            "\u001b[?25hCollecting colorcet==2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/0c/e312b282b0ee08a34c7a5ce703eed988502149be8c90c3b3b79e97e37284/colorcet-2.0.1-py2.py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 54.8MB/s \n",
            "\u001b[?25hCollecting brokenaxes==0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/41/0a8839699a98387d63d027909626920df7a7d6634d2a0a3bc1096626954c/brokenaxes-0.3.1.tar.gz\n",
            "Collecting sacred==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/89/bd8a71138906b146eea621505236f8704a3eeab2b9668aad77691b93fdb8/sacred-0.7.5-py2.py3-none-any.whl (92kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.0MB/s \n",
            "\u001b[?25hCollecting pymongo==3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4a/586826433281ca285f0201235fccf63cc29a30fa78bcd72b6a34e365972d/pymongo-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (3.12.4)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: param>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from colorcet==2.0.1) (1.10.1)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from colorcet==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (1.12.1)\n",
            "Collecting munch<3.0,>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (0.6.2)\n",
            "Collecting colorama>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting jsonpickle<1.0,>=0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/d5/2f47f03d3f64c31b0d7070b488274631d7567c36e81a9f744e6638bb0f0d/jsonpickle-0.9.6.tar.gz (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.6MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (20.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.12.0) (51.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.12.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.4.0)\n",
            "Building wheels for collected packages: brokenaxes, jsonpickle, py-cpuinfo\n",
            "  Building wheel for brokenaxes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for brokenaxes: filename=brokenaxes-0.3.1-cp36-none-any.whl size=4846 sha256=55b8705c49bfb5f09e7fa919ec28e50867310e04e5e552ecfdb423fbefe889de\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/df/12/5b119337c272bbeda8483d0ada967f95f17e214fffc8f5134d\n",
            "  Building wheel for jsonpickle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonpickle: filename=jsonpickle-0.9.6-cp36-none-any.whl size=29464 sha256=cc901cd4ebe0e78ada80a4e1308a0d7e630cf0dc9b96cfa9c0e2036b3cc5b03c\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/41/8ce98f4737a9ff61b1bf2673f2abfe66a6a43ad6e91d2c9736\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20072 sha256=68f0d23975320fac60fbb0f5331717b296c078e8e741757a001f69d330bb27b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built brokenaxes jsonpickle py-cpuinfo\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement tensorboard~=2.4, but you'll have tensorboard 1.12.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, keras-applications, tensorflow-gpu, tensorflow-probability, matplotlib, colorcet, brokenaxes, munch, colorama, jsonpickle, py-cpuinfo, sacred, pymongo\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: pymongo 3.11.2\n",
            "    Uninstalling pymongo-3.11.2:\n",
            "      Successfully uninstalled pymongo-3.11.2\n",
            "Successfully installed brokenaxes-0.3.1 colorama-0.4.4 colorcet-2.0.1 jsonpickle-0.9.6 keras-applications-1.0.8 matplotlib-3.1.0 munch-2.5.0 py-cpuinfo-7.0.0 pymongo-3.8.0 sacred-0.7.5 tensorboard-1.12.2 tensorflow-gpu-1.12.0 tensorflow-probability-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1hUvWAlPTer",
        "outputId": "7e02a70b-bc98-4c5f-ddd6-ec77b73b8f6e"
      },
      "source": [
        "%cd /content/bcnn/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bcnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T64XJG-SPb33",
        "outputId": "7fe94b34-da05-4bc2-8329-d2444d1f40e6"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bayesian_unet.py  dropout_unet.py\tLICENSE    requirements.txt   train.py\n",
            "bayesian_vnet.py  dropout_vnet.py\tmodel.py   stats.py\t      unc.PNG\n",
            "configs\t\t  generate_toy_data.py\tpavpu.py   test.py\t      utils.py\n",
            "dataset.py\t  groupnorm.py\t\tREADME.md  toy_train_test.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6rOzTOJ6DqH",
        "outputId": "0e902269-d4d5-4cfe-fff6-ec194cfbd9fa"
      },
      "source": [
        "import skimage.io\n",
        "import numpy as np\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/cells/test/masks/*0.png'\n",
        "img_ = skimage.io.imread(path)\n",
        "print(np.unique(img_))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0 255]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3GIQ110LPF"
      },
      "source": [
        "import os\n",
        "path = '/content/weights/bayesian'\n",
        "for i in os.listdir(path):\n",
        "  os.remove(os.path.join(path, i))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVbkNK9wPDLf",
        "outputId": "b4753df1-8578-400d-d949-98f0f94670bb"
      },
      "source": [
        "!python train.py with configs/toy_config.json \"num_gpus\"=1 \"batch_size\"=8 \"prior_std\"=4.5"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING - utils - No observers have been added to this run\n",
            "INFO - utils - Running command 'train'\n",
            "INFO - utils - Started\n",
            "loading data...\n",
            "in save_train_data\n",
            "getting model...\n",
            "input shape is (256, 1, 256)\n",
            "train shape is (502, 256, 1, 256)\n",
            "val shape is (100, 256, 1, 256)\n",
            "train_targets shape is (128512, 256, 1)\n",
            "val_targets shape is (25600, 256, 1)\n",
            "input shape is (256, 256, 1)\n",
            "train shape is (502, 256, 256, 1)\n",
            "val shape is (100, 256, 256, 1)\n",
            "train_targets shape is (502, 256, 256, 1)\n",
            "val_targets shape is (100, 256, 256, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:102: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:102: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "Model: \"model\"\n",
            "_______________________________________________________________________________________________________________________________\n",
            "Layer (type)                             Output Shape                Param #         Connected to                              \n",
            "===============================================================================================================================\n",
            "input_1 (InputLayer)                     [(None, 256, 256, 1)]       0                                                         \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                          (None, 256, 256, 16)        160             input_1[0][0]                             \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization (GroupNormalization) (None, 256, 256, 16)        32              conv2d[0][0]                              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)                        (None, 256, 256, 16)        2320            group_normalization[0][0]                 \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_1 (GroupNormalizatio (None, 256, 256, 16)        32              conv2d_1[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)             (None, 128, 128, 16)        0               group_normalization_1[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)                        (None, 128, 128, 32)        4640            max_pooling2d[0][0]                       \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_2 (GroupNormalizatio (None, 128, 128, 32)        64              conv2d_2[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)                        (None, 128, 128, 32)        9248            group_normalization_2[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_3 (GroupNormalizatio (None, 128, 128, 32)        64              conv2d_3[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)           (None, 64, 64, 32)          0               group_normalization_3[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)                        (None, 64, 64, 64)          18496           max_pooling2d_1[0][0]                     \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_4 (GroupNormalizatio (None, 64, 64, 64)          128             conv2d_4[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)                        (None, 64, 64, 64)          36928           group_normalization_4[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_5 (GroupNormalizatio (None, 64, 64, 64)          128             conv2d_5[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)           (None, 32, 32, 64)          0               group_normalization_5[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)                        (None, 32, 32, 128)         73856           max_pooling2d_2[0][0]                     \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_6 (GroupNormalizatio (None, 32, 32, 128)         256             conv2d_6[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)                        (None, 32, 32, 128)         147584          group_normalization_6[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_7 (GroupNormalizatio (None, 32, 32, 128)         256             conv2d_7[0][0]                            \n",
            "_______________________________________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)             (None, 64, 64, 128)         0               group_normalization_7[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout (Conv2DFlipout)           (None, 64, 64, 64)          65600           up_sampling2d[0][0]                       \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_8 (GroupNormalizatio (None, 64, 64, 64)          128             conv2d_flipout[0][0]                      \n",
            "_______________________________________________________________________________________________________________________________\n",
            "concatenate (Concatenate)                (None, 64, 64, 128)         0               group_normalization_5[0][0]               \n",
            "                                                                                     group_normalization_8[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_9 (GroupNormalizatio (None, 64, 64, 128)         256             concatenate[0][0]                         \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_1 (Conv2DFlipout)         (None, 64, 64, 64)          147520          group_normalization_9[0][0]               \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_10 (GroupNormalizati (None, 64, 64, 64)          128             conv2d_flipout_1[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_2 (Conv2DFlipout)         (None, 64, 64, 64)          73792           group_normalization_10[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_11 (GroupNormalizati (None, 64, 64, 64)          128             conv2d_flipout_2[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)           (None, 128, 128, 64)        0               group_normalization_11[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_3 (Conv2DFlipout)         (None, 128, 128, 32)        16416           up_sampling2d_1[0][0]                     \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_12 (GroupNormalizati (None, 128, 128, 32)        64              conv2d_flipout_3[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)              (None, 128, 128, 64)        0               group_normalization_3[0][0]               \n",
            "                                                                                     group_normalization_12[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_13 (GroupNormalizati (None, 128, 128, 64)        128             concatenate_1[0][0]                       \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_4 (Conv2DFlipout)         (None, 128, 128, 32)        36896           group_normalization_13[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_14 (GroupNormalizati (None, 128, 128, 32)        64              conv2d_flipout_4[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_5 (Conv2DFlipout)         (None, 128, 128, 32)        18464           group_normalization_14[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_15 (GroupNormalizati (None, 128, 128, 32)        64              conv2d_flipout_5[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)           (None, 256, 256, 32)        0               group_normalization_15[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_6 (Conv2DFlipout)         (None, 256, 256, 16)        4112            up_sampling2d_2[0][0]                     \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_16 (GroupNormalizati (None, 256, 256, 16)        32              conv2d_flipout_6[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)              (None, 256, 256, 32)        0               group_normalization_1[0][0]               \n",
            "                                                                                     group_normalization_16[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_17 (GroupNormalizati (None, 256, 256, 32)        64              concatenate_2[0][0]                       \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_7 (Conv2DFlipout)         (None, 256, 256, 16)        9232            group_normalization_17[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_18 (GroupNormalizati (None, 256, 256, 16)        32              conv2d_flipout_7[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_8 (Conv2DFlipout)         (None, 256, 256, 16)        4624            group_normalization_18[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "group_normalization_19 (GroupNormalizati (None, 256, 256, 16)        32              conv2d_flipout_8[0][0]                    \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_9 (Conv2DFlipout)         (None, 256, 256, 1)         289             group_normalization_19[0][0]              \n",
            "_______________________________________________________________________________________________________________________________\n",
            "conv2d_flipout_10 (Conv2DFlipout)        (None, 256, 256, 1)         3               conv2d_flipout_9[0][0]                    \n",
            "===============================================================================================================================\n",
            "Total params: 672,260\n",
            "Trainable params: 672,260\n",
            "Non-trainable params: 0\n",
            "_______________________________________________________________________________________________________________________________\n",
            "fitting model...\n",
            "Train on 502 samples, validate on 100 samples\n",
            "Epoch 1/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 753026.1139 - acc: 0.8248\n",
            "Epoch 00001: val_loss improved from inf to 747049.00000, saving model to /content/weights/bayesian/bayesian-01-0.819-747049.h5\n",
            "Current KL Weight is 0.5\n",
            "502/502 [==============================] - 35s 69ms/sample - loss: 752956.8690 - acc: 0.8251 - val_loss: 747049.0000 - val_acc: 0.8192\n",
            "Epoch 2/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 741454.3417 - acc: 0.8483\n",
            "Epoch 00002: val_loss improved from 747049.00000 to 735499.70250, saving model to /content/weights/bayesian/bayesian-02-0.819-735500.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 21s 42ms/sample - loss: 741385.3586 - acc: 0.8475 - val_loss: 735499.7025 - val_acc: 0.8192\n",
            "Epoch 3/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 729922.7722 - acc: 0.8470\n",
            "Epoch 00003: val_loss improved from 735499.70250 to 723986.05250, saving model to /content/weights/bayesian/bayesian-03-0.819-723986.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 21s 42ms/sample - loss: 729853.9960 - acc: 0.8475 - val_loss: 723986.0525 - val_acc: 0.8192\n",
            "Epoch 4/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 718425.2601 - acc: 0.8481\n",
            "Epoch 00004: val_loss improved from 723986.05250 to 712505.53250, saving model to /content/weights/bayesian/bayesian-04-0.819-712506.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 21s 43ms/sample - loss: 718356.6828 - acc: 0.8475 - val_loss: 712505.5325 - val_acc: 0.8191\n",
            "Epoch 5/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 706960.8448 - acc: 0.8469\n",
            "Epoch 00005: val_loss improved from 712505.53250 to 701058.51500, saving model to /content/weights/bayesian/bayesian-05-0.819-701059.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 43ms/sample - loss: 706892.4666 - acc: 0.8474 - val_loss: 701058.5150 - val_acc: 0.8192\n",
            "Epoch 6/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 695530.3700 - acc: 0.8486\n",
            "Epoch 00006: val_loss improved from 701058.51500 to 689646.00750, saving model to /content/weights/bayesian/bayesian-06-0.819-689646.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 43ms/sample - loss: 695462.2014 - acc: 0.8474 - val_loss: 689646.0075 - val_acc: 0.8191\n",
            "Epoch 7/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 684135.3004 - acc: 0.8477\n",
            "Epoch 00007: val_loss improved from 689646.00750 to 678269.81250, saving model to /content/weights/bayesian/bayesian-07-0.819-678270.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 43ms/sample - loss: 684067.3501 - acc: 0.8474 - val_loss: 678269.8125 - val_acc: 0.8191\n",
            "Epoch 8/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 672777.4839 - acc: 0.8471\n",
            "Epoch 00008: val_loss improved from 678269.81250 to 666931.86000, saving model to /content/weights/bayesian/bayesian-08-0.819-666932.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 672709.7637 - acc: 0.8465 - val_loss: 666931.8600 - val_acc: 0.8192\n",
            "Epoch 9/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 661458.8851 - acc: 0.8472\n",
            "Epoch 00009: val_loss improved from 666931.86000 to 655634.29000, saving model to /content/weights/bayesian/bayesian-09-0.819-655634.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 661391.4089 - acc: 0.8460 - val_loss: 655634.2900 - val_acc: 0.8190\n",
            "Epoch 10/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 650181.6018 - acc: 0.8471\n",
            "Epoch 00010: val_loss improved from 655634.29000 to 644379.10750, saving model to /content/weights/bayesian/bayesian-10-0.819-644379.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 650114.3792 - acc: 0.8473 - val_loss: 644379.1075 - val_acc: 0.8191\n",
            "Epoch 11/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 638947.8478 - acc: 0.8459\n",
            "Epoch 00011: val_loss improved from 644379.10750 to 633168.54000, saving model to /content/weights/bayesian/bayesian-11-0.817-633169.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 638880.8934 - acc: 0.8463 - val_loss: 633168.5400 - val_acc: 0.8175\n",
            "Epoch 12/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 627813.6280 - acc: 0.8473\n",
            "Epoch 00012: val_loss improved from 633168.54000 to 622115.90750, saving model to /content/weights/bayesian/bayesian-12-0.819-622116.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 627747.6205 - acc: 0.8468 - val_loss: 622115.9075 - val_acc: 0.8191\n",
            "Epoch 13/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 616888.5272 - acc: 0.8442\n",
            "Epoch 00013: val_loss improved from 622115.90750 to 611326.73000, saving model to /content/weights/bayesian/bayesian-13-0.819-611327.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 616824.0926 - acc: 0.8447 - val_loss: 611326.7300 - val_acc: 0.8190\n",
            "Epoch 14/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 606271.7480 - acc: 0.8470\n",
            "Epoch 00014: val_loss improved from 611326.73000 to 600893.47500, saving model to /content/weights/bayesian/bayesian-14-0.819-600893.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 606209.4412 - acc: 0.8467 - val_loss: 600893.4750 - val_acc: 0.8192\n",
            "Epoch 15/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 596047.2490 - acc: 0.8475\n",
            "Epoch 00015: val_loss improved from 600893.47500 to 590891.10000, saving model to /content/weights/bayesian/bayesian-15-0.819-590891.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 595987.5142 - acc: 0.8473 - val_loss: 590891.1000 - val_acc: 0.8191\n",
            "Epoch 16/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 586280.5141 - acc: 0.8451\n",
            "Epoch 00016: val_loss improved from 590891.10000 to 581375.05500, saving model to /content/weights/bayesian/bayesian-16-0.819-581375.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 586223.6835 - acc: 0.8453 - val_loss: 581375.0550 - val_acc: 0.8189\n",
            "Epoch 17/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 577017.4516 - acc: 0.8457\n",
            "Epoch 00017: val_loss improved from 581375.05500 to 572381.04750, saving model to /content/weights/bayesian/bayesian-17-0.819-572381.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 44ms/sample - loss: 576963.7375 - acc: 0.8459 - val_loss: 572381.0475 - val_acc: 0.8190\n",
            "Epoch 18/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 568285.1351 - acc: 0.8424\n",
            "Epoch 00018: val_loss improved from 572381.04750 to 563926.92000, saving model to /content/weights/bayesian/bayesian-18-0.819-563927.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 568234.6444 - acc: 0.8429 - val_loss: 563926.9200 - val_acc: 0.8190\n",
            "Epoch 19/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 560093.8548 - acc: 0.8464\n",
            "Epoch 00019: val_loss improved from 563926.92000 to 556014.97250, saving model to /content/weights/bayesian/bayesian-19-0.810-556015.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 560046.6006 - acc: 0.8468 - val_loss: 556014.9725 - val_acc: 0.8101\n",
            "Epoch 20/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 552440.0444 - acc: 0.8459\n",
            "Epoch 00020: val_loss improved from 556014.97250 to 548635.61250, saving model to /content/weights/bayesian/bayesian-20-0.815-548636.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 552395.9701 - acc: 0.8459 - val_loss: 548635.6125 - val_acc: 0.8149\n",
            "Epoch 21/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 545309.4526 - acc: 0.8460\n",
            "Epoch 00021: val_loss improved from 548635.61250 to 541769.60000, saving model to /content/weights/bayesian/bayesian-21-0.814-541770.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 545268.4430 - acc: 0.8457 - val_loss: 541769.6000 - val_acc: 0.8138\n",
            "Epoch 22/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 538680.0585 - acc: 0.8457\n",
            "Epoch 00022: val_loss improved from 541769.60000 to 535391.72500, saving model to /content/weights/bayesian/bayesian-22-0.819-535392.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 538641.9639 - acc: 0.8455 - val_loss: 535391.7250 - val_acc: 0.8190\n",
            "Epoch 23/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 532524.7550 - acc: 0.8461\n",
            "Epoch 00023: val_loss improved from 535391.72500 to 529473.11000, saving model to /content/weights/bayesian/bayesian-23-0.811-529473.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 532489.4026 - acc: 0.8455 - val_loss: 529473.1100 - val_acc: 0.8112\n",
            "Epoch 24/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 526813.6709 - acc: 0.8449\n",
            "Epoch 00024: val_loss improved from 529473.11000 to 523982.73625, saving model to /content/weights/bayesian/bayesian-24-0.813-523983.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 526780.8739 - acc: 0.8452 - val_loss: 523982.7363 - val_acc: 0.8134\n",
            "Epoch 25/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 521515.6265 - acc: 0.8462\n",
            "Epoch 00025: val_loss improved from 523982.73625 to 518889.19375, saving model to /content/weights/bayesian/bayesian-25-0.819-518889.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 521485.1985 - acc: 0.8470 - val_loss: 518889.1937 - val_acc: 0.8188\n",
            "Epoch 26/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 516599.5217 - acc: 0.8435\n",
            "Epoch 00026: val_loss improved from 518889.19375 to 514161.78750, saving model to /content/weights/bayesian/bayesian-26-0.819-514162.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 516571.2803 - acc: 0.8439 - val_loss: 514161.7875 - val_acc: 0.8190\n",
            "Epoch 27/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 512035.2001 - acc: 0.8443\n",
            "Epoch 00027: val_loss improved from 514161.78750 to 509770.90625, saving model to /content/weights/bayesian/bayesian-27-0.819-509771.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 512008.9694 - acc: 0.8438 - val_loss: 509770.9062 - val_acc: 0.8187\n",
            "Epoch 28/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 507793.8493 - acc: 0.8415\n",
            "Epoch 00028: val_loss improved from 509770.90625 to 505688.71125, saving model to /content/weights/bayesian/bayesian-28-0.819-505689.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 507769.4600 - acc: 0.8424 - val_loss: 505688.7112 - val_acc: 0.8190\n",
            "Epoch 29/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 503848.5439 - acc: 0.8441\n",
            "Epoch 00029: val_loss improved from 505688.71125 to 501889.10750, saving model to /content/weights/bayesian/bayesian-29-0.819-501889.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 503825.8431 - acc: 0.8445 - val_loss: 501889.1075 - val_acc: 0.8189\n",
            "Epoch 30/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 500174.2339 - acc: 0.8443\n",
            "Epoch 00030: val_loss improved from 501889.10750 to 498348.10375, saving model to /content/weights/bayesian/bayesian-30-0.813-498348.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 500153.0776 - acc: 0.8444 - val_loss: 498348.1038 - val_acc: 0.8133\n",
            "Epoch 31/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 496747.8720 - acc: 0.8455\n",
            "Epoch 00031: val_loss improved from 498348.10375 to 495043.73250, saving model to /content/weights/bayesian/bayesian-31-0.819-495044.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 496728.1285 - acc: 0.8459 - val_loss: 495043.7325 - val_acc: 0.8186\n",
            "Epoch 32/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 493548.3659 - acc: 0.8436\n",
            "Epoch 00032: val_loss improved from 495043.73250 to 491955.80250, saving model to /content/weights/bayesian/bayesian-32-0.810-491956.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 493529.9163 - acc: 0.8437 - val_loss: 491955.8025 - val_acc: 0.8102\n",
            "Epoch 33/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 490556.4698 - acc: 0.8438\n",
            "Epoch 00033: val_loss improved from 491955.80250 to 489066.11125, saving model to /content/weights/bayesian/bayesian-33-0.819-489066.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 490539.2041 - acc: 0.8439 - val_loss: 489066.1113 - val_acc: 0.8187\n",
            "Epoch 34/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 487754.7117 - acc: 0.8422\n",
            "Epoch 00034: val_loss improved from 489066.11125 to 486357.91750, saving model to /content/weights/bayesian/bayesian-34-0.819-486358.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 487738.5308 - acc: 0.8419 - val_loss: 486357.9175 - val_acc: 0.8190\n",
            "Epoch 35/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 485127.2026 - acc: 0.8452\n",
            "Epoch 00035: val_loss improved from 486357.91750 to 483816.30000, saving model to /content/weights/bayesian/bayesian-35-0.819-483816.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 485112.0161 - acc: 0.8451 - val_loss: 483816.3000 - val_acc: 0.8187\n",
            "Epoch 36/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 482659.6104 - acc: 0.8416\n",
            "Epoch 00036: val_loss improved from 483816.30000 to 481427.55875, saving model to /content/weights/bayesian/bayesian-36-0.819-481428.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 482645.3363 - acc: 0.8419 - val_loss: 481427.5588 - val_acc: 0.8189\n",
            "Epoch 37/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 480338.8604 - acc: 0.8459\n",
            "Epoch 00037: val_loss improved from 481427.55875 to 479179.23375, saving model to /content/weights/bayesian/bayesian-37-0.817-479179.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 480325.4263 - acc: 0.8458 - val_loss: 479179.2337 - val_acc: 0.8166\n",
            "Epoch 38/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 478153.2016 - acc: 0.8451\n",
            "Epoch 00038: val_loss improved from 479179.23375 to 477060.29250, saving model to /content/weights/bayesian/bayesian-38-0.819-477060.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 478140.5415 - acc: 0.8444 - val_loss: 477060.2925 - val_acc: 0.8189\n",
            "Epoch 39/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 476091.9607 - acc: 0.8441\n",
            "Epoch 00039: val_loss improved from 477060.29250 to 475060.48125, saving model to /content/weights/bayesian/bayesian-39-0.816-475060.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 476080.0112 - acc: 0.8439 - val_loss: 475060.4813 - val_acc: 0.8164\n",
            "Epoch 40/40\n",
            "496/502 [============================>.] - ETA: 0s - loss: 474145.4345 - acc: 0.8385\n",
            "Epoch 00040: val_loss improved from 475060.48125 to 473170.71125, saving model to /content/weights/bayesian/bayesian-40-0.811-473171.h5\n",
            "Current KL Weight is 1.0\n",
            "502/502 [==============================] - 22s 45ms/sample - loss: 474134.1401 - acc: 0.8396 - val_loss: 473170.7112 - val_acc: 0.8114\n",
            "INFO - utils - Completed after 0:15:16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-pvgqufL0LS"
      },
      "source": [
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "#### Try on Damaé médical Images ####\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geo9hYJ9pRP0",
        "outputId": "389395fa-60ca-44d8-ff2c-5810cac77bfd"
      },
      "source": [
        "%matplotlib inline\n",
        "!python test.py with configs/toy_config.json "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING - utils - No observers have been added to this run\n",
            "INFO - utils - Running command 'test'\n",
            "INFO - utils - Started\n",
            "in save_test_data\n",
            "input shape  (256, 1, 256)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:102: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING - tensorflow - From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:102: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "ERROR - utils - Failed after 0:00:05!\n",
            "Traceback (most recent calls WITHOUT Sacred internals):\n",
            "  File \"test.py\", line 241, in test\n",
            "    weights_path=weights_path)\n",
            "  File \"/content/bcnn/model.py\", line 103, in get_model\n",
            "    model = load_model(input_shape, latest_weights_path, net)\n",
            "  File \"/content/bcnn/model.py\", line 45, in load_model\n",
            "    model.load_weights(weights_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 182, in load_weights\n",
            "    return super(Model, self).load_weights(filepath, by_name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 1367, in load_weights\n",
            "    with h5py.File(filepath, 'r') as f:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 408, in __init__\n",
            "    swmr=swmr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 173, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 88, in h5py.h5f.open\n",
            "OSError: Unable to open file (unable to open file: name = '/content/weights/bayesian/bayesian-39-0.804-6.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "irGp2rBasiHJ",
        "outputId": "9c930442-bdff-40e1-b8ba-d1915d520913"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/cells/validation/images\"\n",
        "for i in os.listdir(path):\n",
        "  if i == '.DS_Store':\n",
        "    os.remove(os.path.join(path, i))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9bfa1c5a63d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/cells/validation/images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jdeZgAJBWXZH",
        "outputId": "f7d4e98d-e0c6-43e9-f171-f8b93ac7c7ca"
      },
      "source": [
        "!pip install tensorflow-probability==0.7.0 matplotlib==3.1.0 colorcet==2.0.1 brokenaxes==0.3.1 sacred==0.7.5 pymongo==3.8.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 15.7MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 229kB/s \n",
            "\u001b[?25hCollecting colorcet==2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/0c/e312b282b0ee08a34c7a5ce703eed988502149be8c90c3b3b79e97e37284/colorcet-2.0.1-py2.py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 55.3MB/s \n",
            "\u001b[?25hCollecting brokenaxes==0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/41/0a8839699a98387d63d027909626920df7a7d6634d2a0a3bc1096626954c/brokenaxes-0.3.1.tar.gz\n",
            "Collecting sacred==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/89/bd8a71138906b146eea621505236f8704a3eeab2b9668aad77691b93fdb8/sacred-0.7.5-py2.py3-none-any.whl (92kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n",
            "\u001b[?25hCollecting pymongo==3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4a/586826433281ca285f0201235fccf63cc29a30fa78bcd72b6a34e365972d/pymongo-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0) (4.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from colorcet==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: param>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from colorcet==2.0.1) (1.10.1)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (20.8)\n",
            "Collecting jsonpickle<1.0,>=0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/d5/2f47f03d3f64c31b0d7070b488274631d7567c36e81a9f744e6638bb0f0d/jsonpickle-0.9.6.tar.gz (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 12.6MB/s \n",
            "\u001b[?25hCollecting colorama>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (0.6.2)\n",
            "Collecting munch<3.0,>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.7.5) (1.12.1)\n",
            "Collecting py-cpuinfo>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: brokenaxes, jsonpickle, py-cpuinfo\n",
            "  Building wheel for brokenaxes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for brokenaxes: filename=brokenaxes-0.3.1-cp36-none-any.whl size=4846 sha256=0cb035e74b98e18c3ee2856c2e9e5791d29b8d82c63acbb4751f3e5cfab4d176\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/df/12/5b119337c272bbeda8483d0ada967f95f17e214fffc8f5134d\n",
            "  Building wheel for jsonpickle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonpickle: filename=jsonpickle-0.9.6-cp36-none-any.whl size=29464 sha256=25cb716121c7bd511b87d3c5f46b32d31d78aba9f9d5a8c922fcf5553b459f22\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/41/8ce98f4737a9ff61b1bf2673f2abfe66a6a43ad6e91d2c9736\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20072 sha256=b3af74a7692e3cad6c2f2c783b342e45b4db67bdd79c70c927e0d3f4eef34823\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built brokenaxes jsonpickle py-cpuinfo\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-probability, matplotlib, colorcet, brokenaxes, jsonpickle, colorama, munch, py-cpuinfo, sacred, pymongo\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: pymongo 3.11.2\n",
            "    Uninstalling pymongo-3.11.2:\n",
            "      Successfully uninstalled pymongo-3.11.2\n",
            "Successfully installed brokenaxes-0.3.1 colorama-0.4.4 colorcet-2.0.1 jsonpickle-0.9.6 matplotlib-3.1.0 munch-2.5.0 py-cpuinfo-7.0.0 pymongo-3.8.0 sacred-0.7.5 tensorflow-probability-0.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AdTw2uvS1Xj",
        "outputId": "9dd32b49-5265-47fa-f9a1-7dcb6c8a920a"
      },
      "source": [
        "!sudo apt-get install cuda-9-0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package cuda-9-0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu6dO0zfV2MW",
        "outputId": "0f5dd585-9863-4fc5-a2bd-d5e151b143be"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (51.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f881758f5ea7863afaef503810dda506a129e19d8db695fb2db7ff6c1688adfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: keras-applications, tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p60mt1fSeDIA"
      },
      "source": [
        "https://github.com/biomedia-mira/stochastic_segmentation_network **** Données Cerveaux ****\n",
        "To download the data, go to this page and follow the instructions provided by the challenge's organisers. Run the following script to preprocess the data (input-dir should contain folders HGG and LGG inside):\n",
        "# python evaluation/preprocessing.py --input-dir <path-to-input-dir> --output-dir <path-to-output-dir>\n",
        "\n",
        "https://github.com/abhinavsagar/uqvi **** DONNEE CERVAUX ****\n",
        "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation\n",
        "The dataset can be downloaded from here.\n",
        "\n",
        "https://github.com/sandialabs/bcnn ****** NE DONNE PAS SES DONNEES Mais liobre utilisation ****\n",
        "\n",
        "#### Cellule ####  Ihab exploite jeu de données cellules / Kaggle \n",
        "https://www.kaggle.com/rexhaif/morphological-postprocessing-on-unet-lb-0-429 \n",
        "\n",
        "# Preprocessing the dataset Kaggle \n",
        "https://www.kaggle.com/dingli/keras-u-net-for-nuclei-segmentation \n",
        "\n",
        "https://github.com/xiaoketongxue/AI-News/blob/master/CV2019.md \n",
        "\n",
        "# sudo apt-get install cuda-9-0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G4Ca3R2erjN"
      },
      "source": [
        "https://github.com/biomedia-mira/stochastic_segmentation_network\n",
        "To download the data, go to this page and follow the instructions provided by the challenge's organisers. Run the following script to preprocess the data (input-dir should contain folders HGG and LGG inside):\n",
        "# python evaluation/preprocessing.py --input-dir <path-to-input-dir> --output-dir <path-to-output-dir>\n",
        "\n",
        "https://github.com/abhinavsagar/uqvi \n",
        "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation\n",
        "The dataset can be downloaded from here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-7I22xfbzP",
        "outputId": "4310af43-780f-4cbf-dbc5-0025f77be5ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorflow==2.3 keras==2.1.5"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.3 in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Collecting keras==2.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.4.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.32.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow==2.3) (51.1.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (3.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.7.4.3)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK5sXIw_dWj9"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "import os\n",
        "import keras\n",
        "from keras.layers import BatchNormalization, MaxPooling2D, UpSampling2D, Dropout, Conv2D, Dense, Input\n",
        "#from Models.losses import weighted_bce_dice_loss, weighted_dice_loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from sklearn.utils import class_weight\n",
        "from keras import layers as L\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "seed = 42\n",
        "random.seed = seed\n",
        "np.random.seed(seed=seed)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq1XGC5nWG4o"
      },
      "source": [
        "train_path = \"/content/drive/MyDrive/Colab Notebooks/cells/train\"\n",
        "validation_path = \"/content/drive/MyDrive/Colab Notebooks/cells/validation\"\n",
        "test_path = \"/content/drive/MyDrive/Colab Notebooks/cells/test\"\n",
        "test_batch_size = 1\n",
        "checkpoint_path = '/content/weights'\n",
        "input_size = (256, 256, 1)  # (height, width) of input_size should be divisible by 32\n",
        "num_class = 2\n",
        "# fine tuning\n",
        "train_batch_size = 12\n",
        "test_batch_size = 12\n",
        "validation_batch_size = 12\n",
        "learning_rate = 1e-4\n",
        "nb_epochs = 20"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BKLJgxsZLlo"
      },
      "source": [
        "def unet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=False, conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal')\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "def CorneaNet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "    #weight_ip = L.Input(shape=(256, 512, 1))\n",
        "\n",
        "    unet_layer1 = unet_layer(inputs, num_filters=16)\n",
        "    unet_layer1 = unet_layer(unet_layer1, num_filters=16)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(unet_layer1)\n",
        "\n",
        "    unet_layer2 = unet_layer(pool1, num_filters=32)\n",
        "    unet_layer2 = unet_layer(unet_layer2, num_filters=32)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(unet_layer2)\n",
        "\n",
        "    unet_layer3 = unet_layer(pool2, num_filters=64)\n",
        "    unet_layer3 = unet_layer(unet_layer3, num_filters=64)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(unet_layer3)\n",
        "\n",
        "    unet_layer4 = unet_layer(pool3, num_filters=128)\n",
        "    unet_layer4 = unet_layer(unet_layer4, num_filters=128)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(unet_layer4)\n",
        "\n",
        "    unet_layer5 = unet_layer(pool4, num_filters=256)\n",
        "    unet_layer5 = unet_layer(unet_layer5, num_filters=256)\n",
        "    drop5 = Dropout(0.5, trainable=True)(unet_layer5)\n",
        "\n",
        "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
        "    unet_layer_after_up6 = unet_layer(up6, num_filters=128)\n",
        "    merge6 = concatenate([unet_layer4, unet_layer_after_up6])\n",
        "    unet_layer6 = unet_layer(merge6, num_filters=128)\n",
        "    unet_layer6 = unet_layer(unet_layer6, num_filters=128)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(unet_layer6)\n",
        "    unet_layer_after_up7 = unet_layer(up7, num_filters=64)\n",
        "    merge7 = concatenate([unet_layer3, unet_layer_after_up7])\n",
        "    unet_layer7 = unet_layer(merge7, num_filters=64)\n",
        "    unet_layer7 = unet_layer(unet_layer7, num_filters=64)\n",
        "\n",
        "    up8 = UpSampling2D(size=(2, 2))(unet_layer7)\n",
        "    unet_layer_after_up8 = unet_layer(up8, num_filters=32)\n",
        "    merge8 = concatenate([unet_layer2, unet_layer_after_up8])\n",
        "    unet_layer8 = unet_layer(merge8, num_filters=32)\n",
        "    unet_layer8 = unet_layer(unet_layer8, num_filters=32)\n",
        "\n",
        "    up9 = UpSampling2D(size=(2, 2))(unet_layer8)\n",
        "    unet_layer_after_up9 = unet_layer(up9, num_filters=16)\n",
        "    merge9 = concatenate([unet_layer1, unet_layer_after_up9])\n",
        "    unet_layer9 = unet_layer(merge9, num_filters=16)\n",
        "    unet_layer9 = unet_layer(unet_layer9, num_filters=16)\n",
        "    unet_layer_last9 = unet_layer(unet_layer9, num_filters=16)\n",
        "    #dense9 = Conv2D(3, 1, activation='softmax')(unet_layer_last9)\n",
        "    dense9 = Dense(3, activation='softmax')(unet_layer_last9)\n",
        "    model = Model(input=inputs, output=dense9)\n",
        "\n",
        "    #c11 = L.Lambda(lambda x: x / tf.reduce_sum(x, len(x.get_shape()) - 1, True))(dense9)\n",
        "    #c11 = L.Lambda(lambda x: tf.clip_by_value(x, epsilon, 1. - epsilon))(c11)\n",
        "    #c11 = L.Lambda(lambda x: K.log(x))(c11)\n",
        "    #weighted_sm = L.multiply([c11, weight_ip])\n",
        "    #model = Model(inputs=[inputs, weight_ip], outputs=[weighted_sm])\n",
        "\n",
        "    # model = Model(inputs=inputs, outputs=dense9)\n",
        "\n",
        "    if (pretrained_weights):\n",
        "        model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lng02OekZqYE"
      },
      "source": [
        "def generator_adjusted(generator, num_class):\n",
        "\n",
        "    for (img, mask) in generator:\n",
        "        img, mask = adjust_data(img, mask, num_class)\n",
        "        yield img, mask\n",
        "\n",
        "def train_val_gen(train_path, train_batch_size, validation_path, validation_batch_size):\n",
        "    '''\n",
        "\n",
        "    :param train_path:\n",
        "    :param train_batch_size:\n",
        "    :param validation_path:\n",
        "    :param validation_batch_size:\n",
        "    :return:\n",
        "    '''\n",
        "    # generator for training images\n",
        "    train_image_data_gen_args = dict(rescale=1. / 255)  # transformations\n",
        "    train_image_datagen = ImageDataGenerator(**train_image_data_gen_args)\n",
        "    train_image_generator = train_image_datagen.flow_from_directory(train_path, classes=['images'], class_mode=None,  # no labels are returned\n",
        "                                                                    color_mode='rgb', target_size=input_size[:2],\n",
        "                                                                    batch_size=train_batch_size, save_to_dir=None, save_prefix='image',\n",
        "                                                                    seed=1)\n",
        "\n",
        "    # generator for training masks\n",
        "    train_mask_data_gen_args = dict()  # transformations\n",
        "    train_mask_datagen = ImageDataGenerator(**train_mask_data_gen_args)\n",
        "    train_mask_generator = train_mask_datagen.flow_from_directory(train_path,\n",
        "                                                                  classes=['masks'], class_mode=None,  # no labels are returned\n",
        "                                                                  color_mode='grayscale', target_size=input_size[:2],\n",
        "                                                                  batch_size=train_batch_size, save_to_dir=None, save_prefix='mask',\n",
        "                                                                  seed=1)\n",
        "\n",
        "    # generator for validation images\n",
        "    validation_image_data_gen_args = dict(rescale=1. / 255)  # transformations\n",
        "    validation_image_datagen = ImageDataGenerator(**validation_image_data_gen_args)\n",
        "    validation_image_generator = validation_image_datagen.flow_from_directory(validation_path, classes=['images'], class_mode=None,  # no labels are returned\n",
        "                                                                              color_mode='rgb', target_size=input_size[:2],\n",
        "                                                                              batch_size=validation_batch_size, save_to_dir=None, save_prefix='image',\n",
        "                                                                              seed=1)\n",
        "\n",
        "    # generator for validation masks\n",
        "    validation_mask_data_gen_args = dict()  # transformations\n",
        "    validation_mask_datagen = ImageDataGenerator(**validation_mask_data_gen_args)\n",
        "    validation_mask_generator = validation_mask_datagen.flow_from_directory(validation_path,\n",
        "                                                                            classes=['masks'], class_mode=None,  # no labels are returned\n",
        "                                                                            color_mode='grayscale', target_size=input_size[:2],\n",
        "                                                                            batch_size=validation_batch_size, save_to_dir=None, save_prefix='mask',\n",
        "                                                                            seed=1)\n",
        "\n",
        "    # generator for training (image, mask)\n",
        "    train_generator = zip(train_image_generator, train_mask_generator)\n",
        "    train_generator = generator_adjusted(train_generator, num_class)  # modify the masks to be one-hot encoded\n",
        "\n",
        "    # generator for training (image, mask)\n",
        "    validation_generator = zip(validation_image_generator, validation_mask_generator)\n",
        "    validation_generator = generator_adjusted(validation_generator, num_class)  # modify the masks to be one-hot encoded\n",
        "\n",
        "    return train_generator, validation_generator, train_image_generator, validation_image_generator\n",
        "\n",
        "\n",
        "\n",
        "def plot_images(train_image_generator, train_mask_generator):\n",
        "    '''\n",
        "\n",
        "    :param train_image_generator:\n",
        "    :param train_mask_generator:\n",
        "    :return:\n",
        "    '''\n",
        "    fig1, ax1 = plt.subplots(10, 10)\n",
        "    fig2, ax2 = plt.subplots(10, 10)\n",
        "    for i, (img, mask) in enumerate(zip(train_image_generator, train_mask_generator)):\n",
        "        if i+1 > 100:\n",
        "            break\n",
        "        ax1.ravel()[i].imshow(img[:, :, :, 0], 'gray') # [0, :, :, 0]\n",
        "        ax1.ravel()[i].axis('off')\n",
        "        ax2.ravel()[i].imshow(mask[:, :, :, 0], 'gray')\n",
        "        ax2.ravel()[i].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_results(history):\n",
        "    '''\n",
        "\n",
        "    :param history:\n",
        "    :return:\n",
        "    '''\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# generator for test images\n",
        "def predict_on_test_and_plot(test_path, model_name):\n",
        "    '''\n",
        "\n",
        "    :param test_path:\n",
        "    :return:\n",
        "    '''\n",
        "    class_weight = {0: 3,\n",
        "                    1: 3.003,\n",
        "                    2: 1.009}\n",
        "    test_image_data_gen_args = dict(rescale=1. / 255)  # transformations\n",
        "    test_image_datagen = ImageDataGenerator(**test_image_data_gen_args)\n",
        "    test_image_generator = test_image_datagen.flow_from_directory(test_path, classes=['images'], class_mode=None,  # no labels are returned\n",
        "                                                                  color_mode='rgb', target_size=input_size[:2],\n",
        "                                                                  batch_size=1, shuffle=False, seed=1)\n",
        "    # generator for test masks\n",
        "    test_mask_data_gen_args = dict()  # transformations\n",
        "    test_mask_datagen = ImageDataGenerator(**test_mask_data_gen_args)\n",
        "    test_mask_generator = test_mask_datagen.flow_from_directory(test_path,\n",
        "                                                                classes=['masks'], class_mode=None,  # no labels are returned\n",
        "                                                                color_mode='grayscale', target_size=input_size[:2],\n",
        "                                                                batch_size=1, shuffle=False, seed=1)\n",
        "\n",
        "    #model = load_model(os.path.join(train_path, 'checkpoint', model_name), custom_objects={'lossFunc': WeightedLoss})\n",
        "    model = load_model(os.path.join('/content/weights', 'checkpoint', model_name), compile=False)\n",
        "    #model.compile(optimizer=Adam(lr=learning_rate),\n",
        "    #              loss=weightedLoss(keras.losses.categorical_crossentropy, class_weight),\n",
        "    #              metrics=['accuracy'])\n",
        "    \n",
        "    num_valid = test_image_generator.samples\n",
        "    results = model.predict_generator(test_image_generator, num_valid, verbose=1)\n",
        "    results = np.argmax(results, axis=3)\n",
        "\n",
        "    '''\n",
        "    model = unet(num_class, input_size=input_size)\n",
        "    model.load_weights(os.path.join(train_path, 'checkpoint', 'unet_segmentation_epiderm.hdf5'))\n",
        "    num_valid = test_image_generator.samples\n",
        "    results = model.predict_generator(test_image_generator, num_valid, verbose=1)\n",
        "    results = np.argmax(results, axis=3)\n",
        "    '''\n",
        "\n",
        "    # Display result\n",
        "    fig1, ax1 = plt.subplots(num_valid, constrained_layout=True)\n",
        "    fig2, ax2 = plt.subplots(num_valid, constrained_layout=True)\n",
        "    for i, (img, mask) in enumerate(zip(test_image_generator, test_mask_generator)):\n",
        "        if i+1 > num_valid:\n",
        "            break\n",
        "        ax1[i].imshow(img[0, :, :, 0], 'gray')  # original\n",
        "        ax1[i].axis('off')\n",
        "        ax2[i].imshow(img[0, :, :, 0], 'gray')  # original\n",
        "        ax2[i].contour(mask[0, :, :, 0], colors='c', linewidths=0.5)\n",
        "        ax2[i].contour(results[i].astype('float'), colors='r', linewidths=0.5)\n",
        "        ax2[i].axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK5WPl20YJA0"
      },
      "source": [
        "def load_pretrained(checkpoint_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs,\n",
        "                    train_image_generator, validation_image_generator):\n",
        "    '''\n",
        "\n",
        "    :param train_path:\n",
        "    :param model:\n",
        "    :param train_generator:\n",
        "    :param validation_generator:\n",
        "    :param validation_batch_size:\n",
        "    :param train_batch_size:\n",
        "    :param nb_epochs:\n",
        "    :return:\n",
        "    '''\n",
        "    model = CorneaNet(pretrained_weights=None, input_size=(256, 256, 1))\n",
        "    #pretrained_weights = os.path.join(train_path, 'checkpoint', 'unet_segmentation_epiderm.hdf5')\n",
        "    #model.load_weights(pretrained_weights)\n",
        "    model.summary()\n",
        "    model.compile(optimizer=Adam(lr=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        os.path.join(checkpoint_path, 'checkpoint', 'cell_seg.hdf5'),\n",
        "        monitor='loss',\n",
        "        verbose=1,\n",
        "        save_best_only=True)\n",
        "    rlr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, verbose=1, mode='max')\n",
        "\n",
        "    history = model.fit_generator(train_generator,\n",
        "                                  steps_per_epoch=train_image_generator.samples // train_batch_size,\n",
        "                                  epochs=nb_epochs,\n",
        "                                  validation_data=validation_generator,\n",
        "                                  validation_steps=validation_image_generator.samples // validation_batch_size,\n",
        "                                  callbacks=[model_checkpoint, rlr])\n",
        "    return history\n",
        "\n",
        "\n",
        "def main(train_path, train_batch_size, validation_path, validation_batch_size, nb_epochs): #, test_path): #model_name):\n",
        "    \n",
        "    model_name = 'cell_seg.hdf5'\n",
        "    train_generator, validation_generator, train_image_generator, validation_image_generator = train_val_gen(train_path, train_batch_size, validation_path, validation_batch_size)\n",
        "    model = load_pretrained(checkpoint_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs, train_image_generator, validation_image_generator)\n",
        "    #model = load_unet(train_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs, train_image_generator, validation_image_generator)\n",
        "    #model = Unet_backbone(train_generator, train_image_generator, validation_generator, validation_image_generator, train_batch_size, validation_batch_size, nb_epochs)\n",
        "    #model = load_weight_map_unet(train_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs, train_image_generator, validation_image_generator)\n",
        "    plot_results(model, precision=False)\n",
        "    predict_on_test_and_plot(train_path, model_name=model_name, test_path=test_path, test_batch_size=test_batch_size)\n",
        "    return model"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tmb0YbYheCQ",
        "outputId": "be366d61-3faf-4264-8c01-7e8aedd9215d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "model_name = 'cell_seg.hdf5'\n",
        "main(train_path, train_batch_size, validation_path, validation_batch_size, nb_epochs)\n",
        "predict_on_test_and_plot(train_path, test_path, model_name=model_name, test_batch_size=test_batch_size, D=img_reduce)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 502 images belonging to 1 classes.\n",
            "Found 502 images belonging to 1 classes.\n",
            "Found 68 images belonging to 1 classes.\n",
            "Found 68 images belonging to 1 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3878: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-840e1ea7917a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cell_seg.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredict_on_test_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_reduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-563e5bd341f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_path, train_batch_size, validation_path, validation_batch_size, nb_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cell_seg.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_image_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_image_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m#model = load_unet(train_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs, train_image_generator, validation_image_generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#model = Unet_backbone(train_generator, train_image_generator, validation_generator, validation_image_generator, train_batch_size, validation_batch_size, nb_epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-563e5bd341f3>\u001b[0m in \u001b[0;36mload_pretrained\u001b[0;34m(checkpoint_path, train_generator, validation_generator, validation_batch_size, train_batch_size, nb_epochs, train_image_generator, validation_image_generator)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     '''\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorneaNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#pretrained_weights = os.path.join(train_path, 'checkpoint', 'unet_segmentation_epiderm.hdf5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#model.load_weights(pretrained_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-32d7ab49fd76>\u001b[0m in \u001b[0;36mCorneaNet\u001b[0;34m(pretrained_weights, input_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#weight_ip = L.Input(shape=(256, 512, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0munet_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0munet_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_layer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-32d7ab49fd76>\u001b[0m in \u001b[0;36munet_layer\u001b[0;34m(inputs, num_filters, kernel_size, strides, activation, batch_normalization, conv_first)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Activation' is not defined"
          ]
        }
      ]
    }
  ]
}